{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6gpY4uUaB3hA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vOOvfoHvB7So"
   },
   "source": [
    "# Agentic System Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jg21iv1-iVli"
   },
   "source": [
    "#### The Fine-Tuned was downloaded from my Google Drive, zipped and saved for use with the Agentic System. The Google Colab Note book shows the steps from installing the libraries and importing the necessary libraries. Then we went ahead to unzip the model, load it to Google Colab along with the base model, Tokenizers and Adapters. The knowledge base is also uploaded to the Colab environment, and then the Agentic System is implemented. The step by step process is stated below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x0bCrJdblaUi"
   },
   "outputs": [],
   "source": [
    "# Install the required Python libraries for LLM inference, PEFT, evaluation (BLEU, ROUGE), PDF processing, and UI via Gradio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BBWWgSVgBfKn",
    "outputId": "71881c20-ffbf-451c-85ba-1a90f94de305"
   },
   "outputs": [],
   "source": [
    "!pip install datasets transformers bitsandbytes accelerate peft torch safetensors faiss-cpu chardet trl tensorboard gradio rank_bm25 pymupdf sentence-transformers rouge-score\n",
    "!pip install pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5_zg0NaFiqv1"
   },
   "outputs": [],
   "source": [
    "# Import the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "siV_uZDSiq53",
    "outputId": "d5578776-987d-4d74-a658-8907f031e642"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from google.colab import files\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "import zipfile\n",
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "import faiss\n",
    "from rank_bm25 import BM25Okapi\n",
    "import numpy as np\n",
    "import gradio as gr\n",
    "from huggingface_hub import login\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "import pdfplumber\n",
    "from pathlib import Path\n",
    "from peft import PeftModel, PeftConfig\n",
    "import random\n",
    "from huggingface_hub import login\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sEi3v1NQiwhh"
   },
   "source": [
    "### Upload the zipped model to Google colab and unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OsjrxvYxmlF5"
   },
   "outputs": [],
   "source": [
    "#File Upload the Zipped Fine-Tuned Model for unzipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "R4ZrgITbiHYh",
    "outputId": "e205f638-a6eb-47fb-dd80-f08e792cf701"
   },
   "outputs": [],
   "source": [
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lfLZ6Yg4NVBb"
   },
   "outputs": [],
   "source": [
    "# Setting the Path of the ZIP File and the Extracting the ZIP file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fz4IkBiJiHcg"
   },
   "outputs": [],
   "source": [
    "zip_path = \"/content/fine-tuned-llama-3.2-3B_instruct-1.zip\"\n",
    "extract_dir = \"unzipped_model\"\n",
    "os.makedirs(extract_dir, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X4MrFhKXiHgJ"
   },
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L6rkg4QLiHjg",
    "outputId": "ddbf4998-2547-40ff-f031-18a84c15d647"
   },
   "outputs": [],
   "source": [
    "print(f' Successfully extracted model to {extract_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-CzEgFHPlmzR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337,
     "referenced_widgets": [
      "2041365575ec4d02a4738ea9bae31686",
      "9405bb8b373b431f8946e305f4c19bda",
      "80765429b0f3405aa25eed02af09231a",
      "6a016baf52fd4f72a3e38ce5c172704c",
      "be15b55be711486fb4390e6111925be5",
      "0b798a6c25f14842b17acc7e281295df",
      "bf539eeeb8154e709b72c031beec47d5",
      "19b9841cd1f141a4ab1ad0d2dad9078f",
      "d78b5cdea421482290bc2e45006a8afa",
      "2b03af802eb44f4e996f5d0c1c2d341a",
      "2addf89d26b24af999313480b577de8f",
      "1aacfea971824aa7ad4177c97039e63b",
      "6f59f0ce3e5b4e418eeaa8f3e521cffd",
      "d89dbc92bd064599a7026e30154c1b86",
      "d3d410ce5b4e41bbbad415aa97dee9e8",
      "dc2cf303326948a9bd27cccbfefada13",
      "984a6fe04f464df5ab9191052824440b",
      "a580e6528dcb47d4b599909b814dd25f",
      "2502df9e36a44bb4933a3263aa5c9d13",
      "526baa44b7914beab8f6d2bd1bcc6283",
      "bbe798d7956f4b91ab1465f0a37c86c9",
      "53b4f409bf344ff1a3c2ebc3955c7bc0",
      "8a9151a31aee4a7286e0f9e63cfde352",
      "3b72b50530334c7f80f1bdbee04e7a9e",
      "4edb2ec142a443529e7ef509aa85a9e7",
      "7d54899cbf314a7092442472eb083bf7",
      "72f0a626556b4ecfbf0fdc1ceae160c6",
      "cbe40e127d5543129e21790942f77097",
      "a502d103592340b3af5d80b215d598f6",
      "efcb044b10de430c8691fff8a15d3b35",
      "8ec8426a62e94847b17d8371063d8cb8",
      "ff798d84d793415095e0ca4ea84554fc",
      "f7684adb58ca457a867224853d3775b2",
      "b2a8b598d035499aa9db79745ce92f60",
      "a687b998f13f45cc8904733582528b0e",
      "6309ea2f3e6f4e23b82f0abe52379c32",
      "3de38736b0354094b259e91e06bd95e6",
      "725833c96ed14c77b0ce17c9955b643a",
      "e2ca7571846b41e3ac4954fffa02c828",
      "3cc0a2e6b23d443b9b891dd67e4a2c23",
      "ba959014570048a3ad5cd0d3eea3eeca",
      "ab62bbca8557435e9c3cf0667509b626",
      "a2d33415b3dd459386e44eb0d0b18628",
      "3fe7205eb12c49a1b83049d77a5a2f4f",
      "737744dcef034cfb8e816c9a4d5ae744",
      "70079630e4f34c8f8f1afcf26e913a3d",
      "188a6300e14c4d379581e9f9e2f1e41e",
      "86f859aa863647d5ac849c89fca442ec",
      "485b7a6db72f47a4a354b94c95ef537a",
      "40c993a98fb04c2c81d6bf5ce177fd00",
      "2fb813dd70f14bf3a3db82447959a7a9",
      "9b06dd0db29f450a85f8743ad3d5abce",
      "ec2af9748d4d47a0a6d42af6a5f071f2",
      "57e1cbea392948bebbda7f1e96c2e35f",
      "37c8307669bb49af91562dbec29b6f39",
      "0814d78a95c24e0bb08778f62580c5a9",
      "60027d93630d49eb8de57b33b18e7700",
      "85425d6661574ad2bba81846b8ee52a6",
      "b53351ca265b45d896597e6522a181d8",
      "35ec0e5b01c94e47970e33f1d7581ccf",
      "751e77875ea84143829af4b22a6bcf1d",
      "3e05ac95a8d74e10937721be5e512754",
      "44a72ddb5ddb40968829008f16c8e90a",
      "61d6cd93621544ba9540033119b18ae3",
      "1a64d0b6241f43a4bdf545ad0926b705",
      "5eb9936dd22f415eb7e1887a717b3cd2",
      "14006562514a404b9ad5fc67c789ebdf",
      "89c0226e31654dbb84b52d37ff7bbea1",
      "ea015ed5a74642bfa675911b1508f3d4",
      "bb24a24a90fa4863be3bcb8bd60ca094",
      "391502ba0fe14ad1aa95956ce78d12ac",
      "bd9d6110a9e54ece8373110660dc337c",
      "65d5e908286644138c0509099177f635",
      "c44fcc0f344d40a6b1497c058082a7f4",
      "8ced45b93204405c9d94d8d9801bb5c2",
      "a169edd1a8f041419aefd361e2fb755b",
      "24f20c7422844631be0967ebc5599482",
      "e5c433d0807346b2a1682d938fe3969a",
      "7df1643ee7c34bef9a8e05dd64d0e8ef",
      "cd5e11e9cdbb49439c31ee0b9854ac25",
      "0c74207bcb674d1a911e7908c15a461a",
      "da693703c83547a78c1746d0ecd98f89",
      "21b440e4782646a4a70a02628bf9c2c6",
      "1f36f1c5abe242b0b57bea57e55601e6",
      "c6348982ae2242c8854885e69c17346c",
      "8e1788390e9948cbbc36c6a25b4baf50",
      "0e2f4dd477c44114ac8f940341bfa6d8",
      "39656d1a5d7048818a8fa396bd629243",
      "4f358d419d634b7286d30579d4df79a1",
      "42edbcad642048a9bbda5839e700d28a",
      "0049e91992d6472cb81c4233cd45c68d",
      "fee9256800404ba48a6a6806501bd515",
      "48d3315495d14d0f9f666b0793773f08",
      "5abfd7a4e97444f290ce64efbaa6501e",
      "424bb77d07b947958cb6df909fc6d001",
      "1f3b76b18bfa4b8796e27b77f2d335be",
      "47ccbc40f91e4347adf9826246ce84a4",
      "86c9d12b3baa4e11b54e82de208a6ed0",
      "0abd06fee3264826afe71e014b4deabd",
      "399f8c87ee9e4e088ddb8da8ee1d44af",
      "587b1e029a5549f7ad85e0944f901c9b",
      "dc22768632164392a4fd31421b85c313",
      "93a5f6924ed545cab1f1d91d8711a495",
      "16a2cc28d9c147a799b4400416545a3c",
      "d9a53587c7da44969b56c593b96311e9",
      "af72271a4b394c8e8292840df0bfbac1",
      "90085917dee344c18bba2a9672dad496",
      "8583209c80e546e3a9d1fe07c371dde9",
      "fb76ee281a404f2bb792945d1215f9fa",
      "1e0daf5278274d4f94c1bc74e14a9d4b"
     ]
    },
    "id": "kMZNbrjGk2mB",
    "outputId": "53606e5b-0d84-4ba0-9e87-3cfac992401f"
   },
   "outputs": [],
   "source": [
    " # Hugging Face Authentication & Model Loading\n",
    "\n",
    "login(\"HF_TOKEN\")\n",
    "\n",
    "# Local LoRA adapter path\n",
    "adapter_path = \"/content/unzipped_model/content/drive/MyDrive/fine-tuned-llama-3.2-3B_instruct-1\"\n",
    "\n",
    "# Load adapter config\n",
    "peft_config = PeftConfig.from_pretrained(adapter_path)\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    peft_config.base_model_name_or_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    token=\"HF_TOKEN\"\n",
    ")\n",
    "\n",
    "# Apply LoRA adapter\n",
    "model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    peft_config.base_model_name_or_path,\n",
    "    token=\"HF_TOKEN\"\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HfFE76wbt3A-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 528,
     "referenced_widgets": [
      "04654ceb38f8488cae801e1b60847bc1",
      "547d7fae2fcc4ce69f91debec4cab6ab",
      "01372c6a44c24b99b6ea14c14d608caa",
      "e91218001c654b98a6922ce745c06c2d",
      "daded5953b3b435e812387c4fec3d9d7",
      "fd6c0ba087ee4c3db0588e7fefc7e8f8",
      "85078f0c35cf4083a23e7f495c8e101d",
      "1a85d7c494b8404f8b5f825ed7a15956",
      "02c5643c462e4c78a7a6a81d3ace3a40",
      "601d054a57f24c04b5082948e42a4c0d",
      "a7bddd05780d41c79f302f64e2101c3b",
      "fd16281530394139bae7003bbcf07354",
      "f83fe718d7e64a1c92b29c4b23b6cd32",
      "d246b5cee66e4286be0b8d8aa5d1356e",
      "048cbd6b9f994d398e78ed5dd584726b",
      "383cd151b38d4361a883dda63d49db92",
      "a3fe71e97f6940dda4ebe402a9748335",
      "4df81e6e7e92430ca995eaf02cc46322",
      "bbeea9357e0c44f9a4948f629d0ab624",
      "9e60af3e41204718a99f96eb7d4cd051",
      "4713702b9d434d65abd8f13befaadc14",
      "658bca6de75c49078221a84595eafdba",
      "6b06dc67a81643d19227ebad977a7bdf",
      "2d291ddd87b440979b93bd47e2135409",
      "ed57f44b59df4fabbb929c916ef90437",
      "9ffe4e6e5ea2432e8422ab0bf20540ea",
      "e90559b35eaa42a18f5d3c8bb11d9a3b",
      "0570e5df8aea4d4ebfc8f45572ac2a77",
      "04279073c36b4f4bb476c6c331e09f33",
      "05bb301c17a342f39bf6672c00413354",
      "4732d609474a44d5a3b4e48b12648be7",
      "e600f3896fbe4583ab5777f7a4f330c6",
      "680734b210304e63b6707933bc35ff0e",
      "96b2e8cd8bbb491bbb6b84d9f6f405b4",
      "6cca5eab23c44171abe5fc9450bd2cc8",
      "6162d37e3b5744518ab915d94abec951",
      "8fe371efe4874d39ba7fbd3fa4bd65f3",
      "f540b63164b24374b69907d4ef7fb0ef",
      "a79a922807f649928a9d7d558f9af7d9",
      "09e2873f601f4b50988b438e4ec670d8",
      "7bb915b793c949c29eb352605f5bedd6",
      "de33b047ebbf42ac8fd2e616cf875171",
      "ebf81d5267c74c79b3655b0e66c147e7",
      "ac09215743e54fe78049f66216f3ea69",
      "039f4d698eac4052adb127d66cc28c90",
      "28edc1367cfd416e9ce52a1051f8fd7d",
      "51cd8a325a664502ae35fcd1ca70a8bb",
      "9ded9982b1324e60975467f8cefd6240",
      "88fb18e121734da794fe6d567e708478",
      "c4154331762e4c6f9586d8f272c4650a",
      "26f17ad3b071495e86a97f2507371a28",
      "30b0725897c247148a35b78afdf17b08",
      "62b038aed15e43a4aab5cf723e5ca2e7",
      "4ce248bfab59464aa09de9f5bf315b51",
      "b271728287784e8ab8401e1e4621eff5",
      "61ca667788cf4efb9ccba2d973081da6",
      "61fdb56d7c4041b394ea6d6ff0102c59",
      "db5358ce7f1e416e8588b09af6af1270",
      "d9e90826922e4928b8e0b4327396ba37",
      "4380c0a8f9d34bbe832511c188275a67",
      "c261292897794403a5653ea1eedaf06b",
      "75d4c539e6b049b0b35ba3af50dd1b44",
      "6f4344b827ac43cbb82d079d7ffc9489",
      "be5a9d8f0e9442a2ad682d5a232dbd18",
      "53dcff60a7984ec9be92b9cbe238eb73",
      "0342b156fcee41ddb88556eb55acafb0",
      "202cec6618ed41b3934eb26fb1778f93",
      "cf4cd0a1d9cb4c49b89db5048ea908cc",
      "3a85dd06206a41ea9a1e9e2022b67cc7",
      "e724742b5cc54c6da2ee171e6c9989d6",
      "2c73c3b95c8b4ae3b78958942683c41b",
      "a1fa0342111d43e98ab0beedafc15e53",
      "b4a0b298aa914bccacf4c73e6dc0c9d0",
      "ceb5cf2d2a7e4ac9b9acb3c34e85c5b6",
      "1e976c6ecb0e4920bbf2b4e790faab1f",
      "58f62b58dd6b49a3a347ae938b941658",
      "c14e125accca4f418cee490e588127b8",
      "200abb30bbe940c086220e02c90ec7a5",
      "463d70521290492b86cb8f267ee0e42f",
      "11aa15ea58384a2598c19f53a1e5c92e",
      "89372fba0026420aa0e39b87c974cf39",
      "b09687eaec714a26969eb2d7b326b797",
      "dad97b292eaa4162b77241506c9aab92",
      "76a9389fc195438c97b10a9435fe69ab",
      "09ffde329cfd453eb06a6ee7728b87b0",
      "ceb293122dcc47f586d49f63ce2a44e2",
      "3099d836bce5444ab95df56ba4081a43",
      "dd05c1c922ff45bd994a2e10ab4d3c76",
      "2ce7c713c30d451997a9ffb552441bd7",
      "43333cf5444d4274977da9fbe797b4d6",
      "0d33c585c1fc495b8804b3a951d461e1",
      "ddc8206555124ef4aeb4ffd81b2d383a",
      "fa145f8745ac4cb8bac11ffda8f680b5",
      "080e5846af324f2ea5c6fdd14f5040c9",
      "b577225bc6444b2db500b911b61e6b9f",
      "0f060825be7e4527b1948324ab9423e8",
      "1bbf24d6097047e9922fd25bfb977b0e",
      "2b8489dafa27491a91cbe55e1e20a615",
      "b66418eefff1469f8c2e5271fbafba1b",
      "8028f2b59ef74581bd47cb0f57e03973",
      "dbcf6b9db4c448e5b8eb7604ff0ab4cd",
      "2ab3f093dbd340b68614a8069dc16be0",
      "98223c90f38e4fe59bab53a27b8464ab",
      "9b864e6921854ce7b3ca286a0d29fc68",
      "01ef65c022844318a45379d920d544a1",
      "b9559ba476b4407790138cb33a124715",
      "5ca5d7b15c2547a89bf14d8896b06cc2",
      "d168820e31084e3da56b7951e414a464",
      "3eb7adb506254f48add3bb5b602a74b3",
      "1af5590330d340bd90114de9be024752",
      "2d7a9e05d11141279cacee5c00b7961d",
      "6729e7a36b824ee69b9d36584d5568a8",
      "4fe1d148fb2b4327bde9c5be3c5d39ff",
      "486278d6029f4f6d83e35f597050c245",
      "5e03df87114744b7b7bef0199daf20de",
      "42ff199bab4f460cab389ec86bcee502",
      "bac13aeb061c46dcbb0a7ddb1e64d3ea",
      "abbae0446bf94bf9948f5c19b01b4654",
      "c2055ddd3f7e43a1846fbb121457e57e",
      "430a3dca50fb455fa9bfe99eed05639d",
      "30ff9899de9545cc91ceb0f0ec0a44c0"
     ]
    },
    "id": "QfP6e0Cjt3En",
    "outputId": "a5db5254-1931-4ce6-9045-13ce25ea8a49"
   },
   "outputs": [],
   "source": [
    "# Load Sentence-BERT Model for Semantic Search.\n",
    "sentence_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "KTa2s5DBt3IH",
    "outputId": "77fb23b0-131b-4558-bd0f-1d1bbe06abb1"
   },
   "outputs": [],
   "source": [
    "# Upload the knowledge base.\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gZzhx0bgt3L2"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Extract PDFs from uploaded ZIP.\n",
    "\n",
    "zip_path = 'knowledge_base.zip'\n",
    "unzip_folder = '/content/pdf_folder'  # Folder to extract ZIP content into\n",
    "\n",
    "\n",
    "# Unzip the top-level ZIP file\n",
    "os.makedirs(unzip_folder, exist_ok=True)  # Create the output folder if it doesn't exist\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(unzip_folder)  # Extract content of the ZIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D3C2iajRt3Pe",
    "outputId": "91854507-425e-4ccf-9d5f-6f8cc97e20e8"
   },
   "outputs": [],
   "source": [
    "# Extract text from PDFs\n",
    "documents = []\n",
    "document_sources = []\n",
    "\n",
    "for root, _, files in os.walk(unzip_folder):\n",
    "    for filename in files:\n",
    "        if filename.lower().endswith('.pdf'):\n",
    "            pdf_path = os.path.join(root, filename)\n",
    "            doc = fitz.open(pdf_path)\n",
    "            text = \"\".join([page.get_text() for page in doc])\n",
    "            documents.append(text)\n",
    "            document_sources.append(filename)\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents from PDFs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YmkBpMYvpimj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-6e7GZwfpjap"
   },
   "source": [
    "#### Sentence-BERT is used generates semantic embeddings for each document and then converts those embeddings from a PyTorch tensor into a NumPy array. This conversion is necessary to prepare the embeddings for use in FAISS, which enables fast and meaningful document retrieval based on semantic similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sV_8arTIt3TC"
   },
   "outputs": [],
   "source": [
    "# Compute document embeddings for semantic search\n",
    "\n",
    "doc_embeddings = sentence_model.encode(documents, convert_to_tensor=True)\n",
    "document_embeddings_np = doc_embeddings.cpu().detach().numpy()\n",
    "\n",
    "# Setup FAISS index\n",
    "\n",
    "dimension = document_embeddings_np.shape[1]\n",
    "faiss_index = faiss.IndexFlatL2(dimension)\n",
    "faiss_index.add(document_embeddings_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Uege-FNq0Fx"
   },
   "source": [
    "#### BM25Okapi is initialized with a tokenized version of the corpus. It ranks documents based on the presence and frequency of keywords from the user’s query, serving as a traditional keyword-based retrieval system to complement semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fhYb_yE3t3WI"
   },
   "outputs": [],
   "source": [
    "#Initialize BM25 retriever\n",
    "tokenized_corpus = [doc.lower().split() for doc in documents]\n",
    "bm25 = BM25Okapi(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dqtATtETuZ18"
   },
   "source": [
    "####  A hybrid retrieval function combines the results from both FAISS (semantic) and BM25 (keyword) systems. The function selects the top-scoring documents from each method and merges them, enhancing the relevance and diversity of the retrieved content for more accurate downstream responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Fzuva8ct3ZP"
   },
   "outputs": [],
   "source": [
    "#Existing hybrid retrieval function (unchanged for reference)\n",
    "def retrieve_documents_hybrid(query, top_k=3, alpha=0.5, faiss_index=faiss_index, bm25=bm25):\n",
    "    try:\n",
    "        tokenized_query = query.lower().split()\n",
    "        bm25_scores = bm25.get_scores(tokenized_query)\n",
    "        query_embedding = sentence_model.encode(query, convert_to_tensor=True).cpu().detach().numpy()\n",
    "        _, faiss_indices = faiss_index.search(np.array([query_embedding]), k=len(documents))\n",
    "\n",
    "        # Normalize scores\n",
    "        bm25_scores = np.array(bm25_scores)\n",
    "        bm25_scores = (bm25_scores - bm25_scores.min()) / (bm25_scores.max() - bm25_scores.min() + 1e-8)\n",
    "\n",
    "        faiss_scores = np.zeros(len(documents))\n",
    "        for rank, idx in enumerate(faiss_indices[0]):\n",
    "            faiss_scores[idx] = 1 / (rank + 1)  # Simple decay\n",
    "\n",
    "        # Hybrid scoring\n",
    "        hybrid_scores = alpha * bm25_scores + (1 - alpha) * faiss_scores\n",
    "        top_indices = np.argsort(hybrid_scores)[-top_k:][::-1]\n",
    "\n",
    "        return [documents[i] for i in top_indices]\n",
    "    except Exception as e:\n",
    "        print(f\"Error in retrieve_documents_hybrid: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n3rLAImgugnq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XbStGOenumkq"
   },
   "source": [
    "#### A relevance scoring function computes cosine similarity between the query and each retrieved document and calculates keyword overlap. This helps assess how well each document matches the user’s intent, supporting better decision-making on whether to retrieve or generate answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WFddUDQCDle4"
   },
   "outputs": [],
   "source": [
    "# Evaluate relevance using cosine similarity and content coverage\n",
    "def evaluate_relevance(query, retrieved_docs):\n",
    "    try:\n",
    "        if not retrieved_docs:\n",
    "            return 0.0\n",
    "        query_embedding = sentence_model.encode(query, convert_to_tensor=True)\n",
    "        doc_embeddings = sentence_model.encode(retrieved_docs, convert_to_tensor=True)\n",
    "        # Average cosine similarity\n",
    "        cos_sim = util.pytorch_cos_sim(query_embedding, doc_embeddings).mean().item()\n",
    "        # Content coverage: Check if retrieved documents contain query keywords\n",
    "        query_tokens = set(query.lower().split())\n",
    "        coverage_scores = []\n",
    "        for doc in retrieved_docs:\n",
    "            doc_tokens = set(doc.lower().split())\n",
    "            coverage = len(query_tokens.intersection(doc_tokens)) / len(query_tokens) if query_tokens else 0.0\n",
    "            coverage_scores.append(coverage)\n",
    "        avg_coverage = np.mean(coverage_scores) if coverage_scores else 0.0\n",
    "        # Combine cosine similarity and coverage (weighted)\n",
    "        return 0.7 * cos_sim + 0.3 * avg_coverage\n",
    "    except Exception as e:\n",
    "        print(f\"Error in evaluate_relevance: {e}\")\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "paW5JNDnt3cX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EtD6b36dvEPC"
   },
   "source": [
    "####  The agentic decision-making layer analyzes the user’s query to determine how the system should respond—either by retrieving relevant documents, generating a response from scratch, or combining both. It first checks whether the query includes course-related keywords such as \"assessment\", \"syllabus\", or \"Teesside University\", suggesting the need for retrieval. Then it calculates the semantic similarity between the query and the document embeddings using cosine similarity. If the query is contextually close to any document or contains relevant keywords, the system retrieves documents using a hybrid method. Based on how well those documents match the query, it decides whether to just retrieve and generate, or retrieve and enhance. If no strong matches are found, the model will choose to generate a response from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qi9RyfRQusny"
   },
   "outputs": [],
   "source": [
    "# Agentic decision-making layer\n",
    "def agent_decision(query, doc_embeddings):\n",
    "    try:\n",
    "        # Improved keyword list for course-related queries\n",
    "        keywords = [\n",
    "            \"define\", \"explain\", \"what is\", \"list\", \"teesside university\",\n",
    "            \"assessment\", \"course\", \"module\", \"syllabus\", \"learning outcome\",\n",
    "            \"curriculum\", \"structure\", \"msc\", \"computer science\", \"data science\"\n",
    "        ]\n",
    "        query_lower = query.lower()\n",
    "        needs_retrieval = any(keyword in query_lower for keyword in keywords)\n",
    "\n",
    "        # Additional heuristic: Check query similarity to document embeddings\n",
    "        query_embedding = sentence_model.encode(query, convert_to_tensor=True)\n",
    "        doc_similarities = util.pytorch_cos_sim(query_embedding, doc_embeddings).cpu().numpy()\n",
    "        max_doc_similarity = np.max(doc_similarities) if doc_similarities.size > 0 else 0.0\n",
    "\n",
    "        # Trigger retrieval if keywords are present or query is similar to documents\n",
    "        if needs_retrieval or max_doc_similarity > 0.5:\n",
    "            retrieved_docs = retrieve_documents_hybrid(query, top_k=3, alpha=0.5)\n",
    "            retrieval_quality = evaluate_relevance(query, retrieved_docs)\n",
    "            # Adjusted threshold for retrieval quality\n",
    "            if retrieval_quality < 0.65 or not retrieved_docs:\n",
    "                return \"retrieve_and_enhance\", retrieved_docs\n",
    "            return \"retrieve_and_generate\", retrieved_docs\n",
    "        return \"generate_from_scratch\", []\n",
    "    except Exception as e:\n",
    "        print(f\"Error in agent_decision: {e}\")\n",
    "        return \"generate_from_scratch\", []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mXZKlfsKvsEa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jRIN7sbdv5qK"
   },
   "source": [
    "#### The generate_response_with_llama3 function uses the fine-tuned Llama-3 model to generate a detailed and structured response. It constructs a context-aware prompt that incorporates retrieved documents when available. If enhance is enabled, the prompt instructs the model to blend retrieved context with general knowledge, ensuring factual accuracy even when the documents are partial or weak. If not, the model is directed to strictly use the provided context. The prompt is then tokenized and passed to the model for generation. Output is decoded, cleaned, and returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aMOIt0SVvsP6"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Generate response using fine-tuned LLaMA model with improved prompt\n",
    "def generate_response_with_llama3(query, retrieved_docs, enhance=False):\n",
    "    try:\n",
    "        retrieved_context = \"\\n\".join(retrieved_docs) if retrieved_docs else \"No relevant context available.\"\n",
    "        # Enhanced prompt with clear instructions\n",
    "        if enhance:\n",
    "            prompt = (\n",
    "                f\"Context (may be incomplete or partially relevant):\\n{retrieved_context}\\n\\n\"\n",
    "                f\"Question: {query}\\n\\n\"\n",
    "                f\"Instructions: Provide a detailed and accurate response to the question. Use the provided context if relevant, \"\n",
    "                f\"but supplement with general knowledge if the context is insufficient. Ensure the response is coherent, \"\n",
    "                f\"factually correct, and avoids fabricating details not supported by the context. \"\n",
    "                f\"Structure the response clearly with sections if applicable.\\n\\n\"\n",
    "                f\"Answer: \"\n",
    "            )\n",
    "        else:\n",
    "            prompt = (\n",
    "                f\"Context:\\n{retrieved_context}\\n\\n\"\n",
    "                f\"Question: {query}\\n\\n\"\n",
    "                f\"Instructions: Generate a response based on the provided context. Ensure the response is accurate, \"\n",
    "                f\"directly addresses the question, and avoids adding information not supported by the context. \"\n",
    "                f\"Structure the response clearly with sections if applicable.\\n\\n\"\n",
    "                f\"Answer: \"\n",
    "            )\n",
    "\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=300, padding=True).to(model.device)\n",
    "        outputs = model.generate(\n",
    "            inputs['input_ids'],\n",
    "            max_length=512,\n",
    "            num_return_sequences=1,\n",
    "            no_repeat_ngram_size=2,\n",
    "            temperature=0.7,\n",
    "            top_p=0.85\n",
    "        )\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        # Clean up response to remove prompt prefix\n",
    "        response = response.split(\"Answer:\")[-1].strip() if \"Answer:\" in response else response\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"Error in generate_response_with_llama3: {e}\")\n",
    "        return \"Error generating response.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cV4Cx6bnw5qj"
   },
   "source": [
    "#### The merge_responses function combines the generated answer with a summarized view of the documents used in retrieval. If no documents were retrieved, it simply returns the generated text. Otherwise, it formats a readable response that includes both the main answer and brief source excerpts. This promotes transparency and credibility, allowing users to see which sources informed the answer, especially valuable in academic or knowledge-based applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g-2YK1Hxt3f5"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Merge retrieved documents and generated response for enhanced responses\n",
    "def merge_responses(retrieved_docs, generated_text):\n",
    "    try:\n",
    "        if not retrieved_docs:\n",
    "            return generated_text\n",
    "        # Summarize retrieved documents for concise integration\n",
    "        retrieved_summary = \"\\n\".join([f\"- {doc[:100]}...\" for doc in retrieved_docs])\n",
    "        return (\n",
    "            f\"Response:\\n{generated_text}\\n\\n\"\n",
    "            f\"Source Information (Summarized):\\n{retrieved_summary}\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error in merge_responses: {e}\")\n",
    "        return generated_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zf_dKVgjxX1b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iu7DMXcUxmp5"
   },
   "source": [
    " The agent_workflow function orchestrates the full intelligent response pipeline. It first invokes the agentic decision layer to determine whether the user’s query should trigger document retrieval, retrieval plus enhancement, or a full generation from scratch. Based on this decision, it routes the query accordingly:\n",
    "\n",
    "For general or unsupported queries, it uses the LLaMA model alone to generate a standalone response.\n",
    "\n",
    "If relevant documents are retrieved, it either generates a response grounded in those documents or enhances the retrieved content with additional model knowledge.\n",
    "\n",
    "When enhancement is applied, it merges the generated response with a summarized view of the sources, ensuring both clarity and transparency.\n",
    "\n",
    "This function is the central logic controller that enables adaptive, context-aware behavior, reflecting the \"agentic\" intelligence of the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nh002RgoxYAq"
   },
   "outputs": [],
   "source": [
    "# Full agentic workflow\n",
    "def agent_workflow(query, doc_embeddings):\n",
    "    try:\n",
    "        action, retrieved_docs = agent_decision(query, doc_embeddings)\n",
    "        print(f\"Agent Decision: {action}\")\n",
    "\n",
    "        if action == \"generate_from_scratch\":\n",
    "            return generate_response_with_llama3(query, [], enhance=False)\n",
    "        elif action == \"retrieve_and_generate\":\n",
    "            return generate_response_with_llama3(query, retrieved_docs, enhance=False)\n",
    "        else:  # retrieve_and_enhance\n",
    "            # Use retrieved documents with enhancement instructions\n",
    "            enhanced_response = generate_response_with_llama3(query, retrieved_docs, enhance=True)\n",
    "            return merge_responses(retrieved_docs, enhanced_response)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in agent_workflow: {e}\")\n",
    "        return \"Error processing query.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W_jMcRMgt3iX",
    "outputId": "fc471fed-5696-4413-a877-0f4540e2b7ec"
   },
   "outputs": [],
   "source": [
    "# Create Gradio interface\n",
    "interface = gr.Interface(\n",
    "    fn=agent_workflow,\n",
    "    inputs=\"text\",\n",
    "    outputs=\"text\",\n",
    "    title=\"Agentic System for Course Planning and Development\",\n",
    "    description=\"Ask the agent a question, and it will retrieve relevant information and generate a response.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 626
    },
    "id": "-w-g0eE5t3lP",
    "outputId": "033035d9-3bb5-40cb-a12a-fb6883139b77"
   },
   "outputs": [],
   "source": [
    "#Launch the interface (comment out if not running interactively)\n",
    "interface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HLa8vxLst3n-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1eCUQVTdt3q1"
   },
   "outputs": [],
   "source": [
    "# Link to a recorded test video\n",
    "\n",
    "# https://www.loom.com/share/42d803547e454518995ba68d6e32cbc8?sid=508d2e03-995a-4b60-9add-7ff7ee025511"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sPsW0HYFPw8W"
   },
   "source": [
    "## Evaluation of Perplexity, Cosine Similarity, BLEU and Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7gCMRyV_aYtQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "InrtMUswyoMr"
   },
   "source": [
    "#### The set_seed function sets a fixed random seed across Python, NumPy, and PyTorch. This ensures that model behavior, data shuffling, and other stochastic processes produce consistent results across different runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q0naLLZcy4DC"
   },
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"Set random seed for reproducibility across runs.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 414,
     "referenced_widgets": [
      "c3d7d8993d4345f89edd90e61e8db772",
      "09603eab27a2474d9579e2fbac59ee49",
      "cf54282523704c9a9dffef83e73e292f",
      "7c369e0cb0d049239290c580fe67afc9",
      "0ab34c876a20451ea44971c25a0ab650",
      "7cf3d528a9024b4da33f75e16b785e70",
      "7b33cdf320a6445e8a80f6a57b770400",
      "b132d027676c4103a3faebd115a61260",
      "da7515e0e4fb44748f2a9cb7c324482c",
      "e73d316a6bff4be0ad27f4c9f7ac346d",
      "ff81683f744340ff8a842452660d00af"
     ]
    },
    "id": "7-sNlc-kXYxf",
    "outputId": "0628aef1-5233-4d4f-a56b-473601d56b1b"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load spaCy model for tokenization\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load sentence transformer model\n",
    "sentence_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# Authenticate with Hugging Face\n",
    "login(\"HF_TOKEN\")\n",
    "\n",
    "# Define adapter path (use the unzipped location)\n",
    "adapter_path = \"/content/unzipped_model/content/drive/MyDrive/fine-tuned-llama-3.2-3B_instruct-1\"\n",
    "\n",
    "# Verify adapter path exists\n",
    "if not os.path.exists(adapter_path):\n",
    "    raise FileNotFoundError(f\"Adapter path {adapter_path} does not exist. Please check the path.\")\n",
    "expected_files = [\"adapter_config.json\", \"adapter_model.safetensors\"]\n",
    "for f in expected_files:\n",
    "    if not os.path.exists(os.path.join(adapter_path, f)):\n",
    "        raise FileNotFoundError(f\"Required file {f} not found in {adapter_path}\")\n",
    "\n",
    "# Load adapter config\n",
    "peft_config = PeftConfig.from_pretrained(adapter_path)\n",
    "\n",
    "# Load base model\n",
    "try:\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        peft_config.base_model_name_or_path,  # Should be \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        token=\"HF_TOKEN\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Failed to load base model: {e}\")\n",
    "\n",
    "# Apply LoRA adapter\n",
    "model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    peft_config.base_model_name_or_path,\n",
    "    token=\"HF_TOKEN\"\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_text(text):\n",
    "    \"\"\"Tokenize text into words using spaCy.\"\"\"\n",
    "    doc = nlp(text.lower())\n",
    "    return [token.text for token in doc if not token.is_punct]\n",
    "\n",
    "def compute_perplexity(prompt, response, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "    \"\"\"Compute perplexity of the response given the prompt using LLaMA.\"\"\"\n",
    "    if model is None or tokenizer is None:\n",
    "        return float('inf')\n",
    "\n",
    "    try:\n",
    "        inputs = tokenizer(prompt + \" \" + response, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "        attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "            loss = outputs.loss\n",
    "\n",
    "        perplexity = torch.exp(loss).item()\n",
    "        return perplexity\n",
    "    except Exception as e:\n",
    "        print(f\"Error computing perplexity: {e}\")\n",
    "        return float('inf')\n",
    "\n",
    "def generate_response_with_llama3(prompt, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "    \"\"\"Generate response using the fine-tuned LLaMA model.\"\"\"\n",
    "    if model is None or tokenizer is None:\n",
    "        return \"Model or tokenizer not loaded.\"\n",
    "\n",
    "    try:\n",
    "        # Set seed before generation for reproducibility\n",
    "        set_seed(42)\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=512,  # Increased for longer responses\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            do_sample=True\n",
    "        )\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating response: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def evaluate_metrics(prompt, generated_response=None, ground_truth=None, use_generated_response=False):\n",
    "    \"\"\"\n",
    "    Evaluate Perplexity, Cosine Similarity, BLEU, and ROUGE for provided inputs.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): Input prompt\n",
    "        generated_response (str, optional): Provided response; ignored if use_generated_response=True\n",
    "        ground_truth (str): Reference response\n",
    "        use_generated_response (bool): If True, generate response with LLaMA\n",
    "\n",
    "    Returns:\n",
    "        dict: Contains scores\n",
    "    \"\"\"\n",
    "    if not prompt or not ground_truth:\n",
    "        print(\"Prompt and ground truth must be provided.\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Generate response if requested\n",
    "        if use_generated_response:\n",
    "            generated_response = generate_response_with_llama3(prompt)\n",
    "        elif not generated_response:\n",
    "            print(\"Generated response must be provided if use_generated_response=False.\")\n",
    "            return None\n",
    "\n",
    "        results = {}\n",
    "\n",
    "        # Perplexity for ground truth and generated response\n",
    "        perplexity_gt = compute_perplexity(prompt, ground_truth)\n",
    "        perplexity_gen = compute_perplexity(prompt, generated_response)\n",
    "        results['Perplexity_Ground_Truth'] = perplexity_gt\n",
    "        results['Perplexity_Generated'] = perplexity_gen\n",
    "\n",
    "        # Cosine Similarity\n",
    "        prompt_emb = sentence_model.encode(prompt, convert_to_tensor=True)\n",
    "        response_emb = sentence_model.encode(generated_response, convert_to_tensor=True)\n",
    "        cos_sim = util.pytorch_cos_sim(prompt_emb, response_emb).cpu().numpy().item()\n",
    "        results['Cosine_Similarity'] = cos_sim\n",
    "\n",
    "        # BLEU\n",
    "        reference_tokens = [tokenize_text(ground_truth)]\n",
    "        generated_tokens = tokenize_text(generated_response)\n",
    "        smoothie = SmoothingFunction().method4\n",
    "        bleu = sentence_bleu(\n",
    "            reference_tokens,\n",
    "            generated_tokens,\n",
    "            weights=(0.25, 0.25, 0.25, 0.25),\n",
    "            smoothing_function=smoothie\n",
    "        )\n",
    "        results['BLEU'] = bleu\n",
    "\n",
    "        # ROUGE\n",
    "        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "        rouge_scores = scorer.score(ground_truth, generated_response)\n",
    "        results['ROUGE1'] = rouge_scores['rouge1'].fmeasure\n",
    "        results['ROUGE2'] = rouge_scores['rouge2'].fmeasure\n",
    "        results['ROUGEL'] = rouge_scores['rougeL'].fmeasure\n",
    "\n",
    "        # Print results\n",
    "        print(f\"Prompt: {prompt[:100]}...\")\n",
    "        print(f\"Generated: {generated_response[:100]}...\")\n",
    "        print(f\"Ground Truth: {ground_truth[:100]}...\")\n",
    "        print(f\"Perplexity (Generated): {perplexity_gen:.2f}\")\n",
    "        print(f\"Cosine Similarity: {cos_sim:.4f}\")\n",
    "        print(f\"BLEU: {bleu:.4f}\")\n",
    "        print(f\"ROUGE-1: {rouge_scores['rouge1'].fmeasure:.4f}\")\n",
    "        print(f\"ROUGE-2: {rouge_scores['rouge2'].fmeasure:.4f}\")\n",
    "        print(f\"ROUGE-L: {rouge_scores['rougeL'].fmeasure:.4f}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        # Save results\n",
    "        result_df = pd.DataFrame([{\n",
    "            'Prompt': prompt,\n",
    "            'Generated_Response': generated_response,\n",
    "            'Ground_Truth': ground_truth,\n",
    "            'Perplexity_Ground_Truth': perplexity_gt,\n",
    "            'Perplexity_Generated': perplexity_gen,\n",
    "            'Cosine_Similarity': cos_sim,\n",
    "            'BLEU': bleu,\n",
    "            'ROUGE1': rouge_scores['rouge1'].fmeasure,\n",
    "            'ROUGE2': rouge_scores['rouge2'].fmeasure,\n",
    "            'ROUGEL': rouge_scores['rougeL'].fmeasure\n",
    "        }])\n",
    "        #result_df.to_excel(\"evaluation_results.xlsx\", index=False)\n",
    "        print(f\"Results saved to evaluation_results.xlsx\")\n",
    "\n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating metrics: {e}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    # Example inputs\n",
    "    prompt = \"Explain the learning outcomes for Machine Learning (CIS4035-N).\"\n",
    "    ground_truth = \"\"\"Learning Outcomes for Machine Learning (CIS4035-N):\n",
    "               - Apply supervised and unsupervised machine learning algorithms to real-world datasets.\n",
    "               - Evaluate model performance using appropriate metrics and validation techniques.\n",
    "               - Design machine learning pipelines for data preprocessing and feature engineering.\n",
    "               - Critically analyze the ethical and societal impacts of machine learning applications.\"\"\"\n",
    "\n",
    "    # Evaluate metrics using generated response\n",
    "    use_generated_response = True\n",
    "    results = evaluate_metrics(\n",
    "        prompt=prompt,\n",
    "        generated_response=None,  # Ignored since use_generated_response=True\n",
    "        ground_truth=ground_truth,\n",
    "        use_generated_response=use_generated_response\n",
    "    )\n",
    "\n",
    "    if results is not None:\n",
    "        print(\"Successfully evaluated metrics\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UgA-_q4UQQuu"
   },
   "source": [
    "## Evaluation of Hallucination Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4VLsNxodP3hu",
    "outputId": "9d53a209-0582-4217-a4dd-f06e8aa3b041"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load SentenceTransformer\n",
    "sentence_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# Provided data\n",
    "query = \"Explain the learning outcomes for the Machine Learning (CIS4035-N)\"\n",
    "generated_response = \"\"\"Answer This machine learning curriculum prepares students to\n",
    "build and deploy sophisticated algorithms that can analyze complex data sets. Students\n",
    "will study key concepts including supervised models, unsupervised methods, machine learning\n",
    "for computer vision, natural language processing, recommender systems, deep learning, as well\n",
    "as the theory behind these algorithms. The curriculum will also cover the machine design process,\n",
    "model evaluation, data preprocessing, feature engineering, deployment, ethical considerations,\n",
    "business applications, security, privacy, bias, fairness, transparency, diversity, inclusion,\n",
    "big data, cloud computing, distributed computing.\"\"\"\n",
    "\n",
    "retrieved_docs = [\"\"\"You are an academic assistant helping to plan and refine course section\n",
    "structures based on Teesside University guidelines. Use the context below to answer the question\n",
    "clearly. Be brief for simple questions and elaborate where needed. Context: Teesside University\n",
    "Course Specification Guide, MSc Computer Science course. Question: Explain the learning outcomes\n",
    "for the Machine Learning (CIS4035-N) section?\n",
    "Answer: Learning Outcomes for Machine Learning (CIS4035-N):\n",
    "- Apply supervised and unsupervised machine learning algorithms to real-world datasets.\n",
    "- Evaluate model performance using appropriate metrics and validation techniques.\n",
    "- Design machine learning pipelines for data preprocessing and feature engineering.\n",
    "- Critically analyze the ethical and societal impacts of machine learning applications.\"\"\"]\n",
    "\n",
    "# Function to compute hallucination metric\n",
    "def compute_hallucination_metric(query, retrieved_docs, generated_response, similarity_threshold=0.6):\n",
    "    \"\"\"\n",
    "    Compute hallucination metric by checking if generated response contains information\n",
    "    not supported by retrieved documents.\n",
    "\n",
    "    Args:\n",
    "        query (str): Input query\n",
    "        retrieved_docs (list): List of retrieved document texts\n",
    "        generated_response (str): Generated response from the model\n",
    "        similarity_threshold (float): Minimum cosine similarity for a sentence to be considered grounded\n",
    "\n",
    "    Returns:\n",
    "        dict: Contains hallucination rate and details\n",
    "    \"\"\"\n",
    "    try:\n",
    "        #Tokenize generated response into sentences\n",
    "        response_sentences = sent_tokenize(generated_response)\n",
    "\n",
    "        # Encode retrieved documents and response sentences\n",
    "        doc_embeddings = sentence_model.encode(retrieved_docs, convert_to_tensor=True)\n",
    "        response_embeddings = sentence_model.encode(response_sentences, convert_to_tensor=True)\n",
    "\n",
    "        # Compute cosine similarity between each response sentence and retrieved documents\n",
    "        hallucinated_sentences = []\n",
    "        grounded_sentences = []\n",
    "\n",
    "        for i, resp_emb in enumerate(response_embeddings):\n",
    "            similarities = util.pytorch_cos_sim(resp_emb.unsqueeze(0), doc_embeddings).cpu().numpy()\n",
    "            max_similarity = np.max(similarities)\n",
    "\n",
    "            if max_similarity < similarity_threshold:\n",
    "                hallucinated_sentences.append({\n",
    "                    'sentence': response_sentences[i],\n",
    "                    'max_similarity': max_similarity\n",
    "                })\n",
    "            else:\n",
    "                grounded_sentences.append({\n",
    "                    'sentence': response_sentences[i],\n",
    "                    'max_similarity': max_similarity\n",
    "                })\n",
    "\n",
    "        # Compute hallucination rate\n",
    "        total_sentences = len(response_sentences)\n",
    "        hallucination_count = len(hallucinated_sentences)\n",
    "        hallucination_rate = hallucination_count / total_sentences if total_sentences > 0 else 0.0\n",
    "\n",
    "        # Return results\n",
    "        return {\n",
    "            'hallucination_rate': hallucination_rate,\n",
    "            'hallucinated_sentences': hallucinated_sentences,\n",
    "            'grounded_sentences': grounded_sentences,\n",
    "            'total_sentences': total_sentences\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error in compute_hallucination_metric: {e}\")\n",
    "        return {\n",
    "            'hallucination_rate': 0.0,\n",
    "            'hallucinated_sentences': [],\n",
    "            'grounded_sentences': [],\n",
    "            'total_sentences': 0\n",
    "        }\n",
    "\n",
    "# Evaluate hallucination for a single query\n",
    "def evaluate_hallucination_single(query, retrieved_docs, generated_response, similarity_threshold=0.6):\n",
    "    \"\"\"\n",
    "    Evaluate hallucination metric for a single query.\n",
    "\n",
    "    Args:\n",
    "        query (str): Input query\n",
    "        retrieved_docs (list): List of retrieved document texts\n",
    "        generated_response (str): Generated response\n",
    "        similarity_threshold (float): Minimum cosine similarity for grounding\n",
    "\n",
    "    Returns:\n",
    "        dict: Hallucination metric results\n",
    "    \"\"\"\n",
    "    result = compute_hallucination_metric(query, retrieved_docs, generated_response, similarity_threshold)\n",
    "\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Hallucination Rate: {result['hallucination_rate']:.2f}\")\n",
    "    print(f\"Hallucinated Sentences: {result['hallucinated_sentences']}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    return result\n",
    "\n",
    "# Evaluate hallucination for the Generated response\n",
    "result = evaluate_hallucination_single(query, retrieved_docs, generated_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zQjap8c4P3kv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yCx-bWACQfKV"
   },
   "source": [
    "##  Evaluation of Hits@ Top K Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jB9M_nWOzgfs"
   },
   "outputs": [],
   "source": [
    "# upload the knowledge base (knowledge_base.zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "HefRl4MgP3tO",
    "outputId": "3269fdb4-4dcf-4222-b80b-d1f9fec935e3"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()  # Upload a ZIP file containing PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C9W9KHt0Qhjl",
    "outputId": "92e3063f-8b6b-4e4e-e963-174ca6cca288"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "# Path to the uploaded ZIP file and the folder to extract PDFs\n",
    "zip_path = 'knowledge_base.zip'\n",
    "unzip_folder = '/content/pdf_folder'\n",
    "\n",
    "# Unzip the top-level ZIP file\n",
    "os.makedirs(unzip_folder, exist_ok=True)\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(unzip_folder)  # Extract content of the ZIP\n",
    "\n",
    "print(f\"Unzipped the top-level file into {unzip_folder}\")\n",
    "\n",
    "# Check the extracted contents (usually a folder)\n",
    "extracted_files = os.listdir(unzip_folder)\n",
    "print(f\"Files inside extracted folder: {extracted_files}\")\n",
    "\n",
    "\n",
    "nested_folder = os.path.join(unzip_folder, extracted_files[0])\n",
    "\n",
    "# Unzip the nested folder containing PDFs\n",
    "if os.path.isdir(nested_folder):\n",
    "    nested_pdf_folder = nested_folder  # Folder where the PDFs are stored\n",
    "    print(f\"Found nested folder: {nested_pdf_folder}\")\n",
    "else:\n",
    "    print(f\"No nested folder found. Ensure the ZIP contains a folder.\")\n",
    "    nested_pdf_folder = unzip_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jaz_qtIHQhnX",
    "outputId": "5a4096c9-628a-456b-99fa-175a07232b5c"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import zipfile\n",
    "import os\n",
    "import pdfplumber\n",
    "from pathlib import Path\n",
    "\n",
    "# Load sentence transformer model\n",
    "sentence_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "def extract_pdf_text(pdf_path):\n",
    "    \"\"\"Extract text from a PDF file using pdfplumber.\"\"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            text = \"\"\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text() or \"\"\n",
    "                text += page_text\n",
    "        return text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from {pdf_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def load_rag_data(zip_path=\"knowledge_base.zip\"):\n",
    "    \"\"\"Load and extract text from PDFs in knowledge_base.zip, handling nested folders.\"\"\"\n",
    "    try:\n",
    "        # Extract the zip file\n",
    "        unzip_folder = \"temp_rag_data\"\n",
    "        os.makedirs(unzip_folder, exist_ok=True)\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(unzip_folder)\n",
    "\n",
    "        # find all PDF files\n",
    "        pdf_files = list(Path(unzip_folder).rglob(\"*.pdf\"))\n",
    "        if not pdf_files:\n",
    "            raise FileNotFoundError(\"No PDF files found in knowledge_base.zip or its subfolders\")\n",
    "\n",
    "        # Extract text from each PDF\n",
    "        documents = []\n",
    "        for pdf_file in pdf_files:\n",
    "            text = extract_pdf_text(pdf_file)\n",
    "            if text:\n",
    "                documents.append({'file': str(pdf_file.relative_to(unzip_folder)), 'text': text})\n",
    "\n",
    "        if not documents:\n",
    "            raise ValueError(\"No valid text extracted from PDFs\")\n",
    "\n",
    "        return documents\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset from {zip_path}: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        # Clean up extracted files\n",
    "        if os.path.exists(unzip_folder):\n",
    "            import shutil\n",
    "            shutil.rmtree(unzip_folder)\n",
    "\n",
    "def compute_top_k_accuracy(prompt, target_response, top_k=3):\n",
    "    \"\"\"Compute Top-k Retrieval Accuracy using PDFs from knowledge_base.zip.\"\"\"\n",
    "    # Load documents\n",
    "    documents = load_rag_data()\n",
    "    if documents is None:\n",
    "        print(\"Documents not loaded. Cannot compute Top-k Accuracy.\")\n",
    "        return 0.0\n",
    "\n",
    "    try:\n",
    "        # Prepare document texts and metadata\n",
    "        doc_texts = [doc['text'] for doc in documents]\n",
    "        doc_files = [doc['file'] for doc in documents]\n",
    "\n",
    "        # Ensure target_response is considered (as a pseudo-document if needed)\n",
    "        doc_texts.append(target_response)\n",
    "        doc_files.append(\"target_response\")\n",
    "\n",
    "        # Encode prompt and document texts\n",
    "        prompt_emb = sentence_model.encode(prompt, convert_to_tensor=True)\n",
    "        doc_embs = sentence_model.encode(doc_texts, convert_to_tensor=True)\n",
    "\n",
    "        # Compute cosine similarities\n",
    "        similarities = util.pytorch_cos_sim(prompt_emb, doc_embs).cpu().numpy().flatten()\n",
    "\n",
    "        # Get indices of top-k most similar documents\n",
    "        top_k_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "\n",
    "        # Check if target_response is in top-k\n",
    "        target_index = len(doc_texts) - 1  # Index of target_response\n",
    "        is_relevant = target_index in top_k_indices\n",
    "\n",
    "        # Get top-k document details\n",
    "        top_k_docs = [(doc_files[i], similarities[i]) for i in top_k_indices]\n",
    "\n",
    "        # Save result\n",
    "        result_df = pd.DataFrame([{\n",
    "            'Prompt': prompt,\n",
    "            'Target_Response': target_response,\n",
    "            'Top_k_Accuracy': 1.0 if is_relevant else 0.0,\n",
    "            'Top_k': top_k,\n",
    "            'Top_k_Documents': str(top_k_docs)\n",
    "        }])\n",
    "        result_df.to_excel(\"top_k_results.xlsx\", index=False)\n",
    "        print(f\"Top-k Accuracy result saved to top_k_results.xlsx\")\n",
    "\n",
    "        # Print top-k documents\n",
    "        print(f\"Top-{top_k} Documents (file, similarity):\")\n",
    "        for doc, sim in top_k_docs:\n",
    "            print(f\"  {doc}: {sim:.4f}\")\n",
    "\n",
    "        return 1.0 if is_relevant else 0.0\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error computing Top-k Accuracy: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "def main():\n",
    "\n",
    "    prompt = \"Explain the learning outcomes for Machine Learning (CIS4035-N).\"\n",
    "\n",
    "    ground_truth = \"\"\"This machine learning curriculum prepares students to build\n",
    "    and deploy sophisticated algorithms that can analyze complex data sets.\n",
    "    Students will study key concepts including supervised models, unsupervised methods,\n",
    "    machine learning for computer vision, natural language processing,\n",
    "    recommender systems, deep learning, as well as the theory behind these algorithms.\n",
    "    The curriculum will also cover the machine design process, model evaluation,\n",
    "    data preprocessing, feature engineering, deployment, ethical considerations, business\n",
    "    applications, security, privacy, bias, fairness, transparency, diversity,\n",
    "    inclusion, big data, cloud computing, distributed computing.\"\"\"\n",
    "\n",
    "    # Compute Top-k Accuracy\n",
    "    top_k_accuracy = compute_top_k_accuracy(prompt, ground_truth, top_k=3)\n",
    "    print(f\"Top-3 Retrieval Accuracy: {top_k_accuracy:.2f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gIEkSwrpLRQ2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0sJlWwC2RNq1"
   },
   "source": [
    "### Visualization of the Quanlitative Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vSV616bvUs25"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "4ytIDRIlXKqY",
    "outputId": "6da25ab9-605f-4992-eb63-731e66f84429"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Data\n",
    "categories = ['Relevance', 'Fluency', 'Completeness', 'Organization']\n",
    "scores = [8.5,7.5,9,8]\n",
    "\n",
    "# Calculate average\n",
    "average = np.mean(scores)\n",
    "\n",
    "# Create the figure and axis\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot the bar chart\n",
    "ax.bar(categories, scores)\n",
    "\n",
    "# Plot the average line\n",
    "ax.axhline(y=average, color='r', linestyle='--', label='Average')\n",
    "\n",
    "# Set title and labels\n",
    "ax.set_title('Evaluation Metric and Average Score ')\n",
    "ax.set_xlabel('Category')\n",
    "ax.set_ylabel('Score')\n",
    "\n",
    "# Legend\n",
    "ax.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1T97XoPhfp1r"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JztJOg-Xfp-T"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mni4IzqmfqBs"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
